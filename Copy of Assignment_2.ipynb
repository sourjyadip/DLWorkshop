{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.0"},"colab":{"name":"Copy of Assignment_2.ipynb","provenance":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"2alEDdwWZHK_","colab_type":"text"},"source":["## CATEGORY 1 - CLASSIFICATION ON IRIS DATASET\n","\n","We will build a classification model on this data using neural network with Tensorflow's Keras API. For simplicity, let’s use ‘petal length’ and ‘petal width’ as the features, and only two species : ‘versicolor’ and ‘virginica’. Download the Iris flower dataset from the webpage. \n","##Prepare the Dataset\n","\n","Import the Iris data set into python and subset the data to keep the relevant rows. Plot the data point in the dataset for the two feature vectors and the two classes."]},{"cell_type":"code","metadata":{"id":"mQzNVsPoZHLD","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1wrz3Su1ZHLK","colab_type":"text"},"source":["## Design a Neural Network\n","\n","We are building a neural network with a single hidden layer. Also, we will set the size of the hidden layer to 6.\n"]},{"cell_type":"code","metadata":{"id":"-rouxh-yZHLM","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"q6x7AvQ9ZHLS","colab_type":"text"},"source":["## Forward Propogation\n","In the forward propagation step, we will use tanh as the first activation function, and sigmoid as the second activation function.\n","\n"]},{"cell_type":"code","metadata":{"id":"CheWWgAeZHLU","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RCbNUvQ-ZHLZ","colab_type":"text"},"source":["## Compute the cost function\n","\n","Compute the cost function to be minimized. We would be calculating the cross-entropy cost (−(ylog(p)+(1−y)log(1−p))).\n","More on the entropy loss from [here](https://ml-cheatsheet.readthedocs.io/en/latest/loss_functions.html)\n"]},{"cell_type":"code","metadata":{"id":"DRYlsLcXZHLb","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3P18ytBZZHLh","colab_type":"text"},"source":["## Backward propagation\n","\n","Compute the backward propagation step in which we calculate the derivatives of the cost function.\n"," Print the cost for every 1000 epochs.\n"]},{"cell_type":"code","metadata":{"id":"IjwCnKaJZHLi","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6v3t5qQ-ZHLs","colab_type":"text"},"source":["## CATEGORY 2 - ACTIVATION FUNCTIONS\n","Now lets create some of the most frequently used activation functions and plot them. Sigmoid function is done for you. Perform in a similar lines for the remaining functions. helpers_04 may be downloaded from the webpage.\n","\n","## Sigmoid (Logistic) Function\n","\n","$$\n","\\sigma(x) = \\frac{1}{1 + e^{-x}}\n","$$\n","\n","$\\sigma$ ranges from (0, 1). When the input $x$ is negative, $\\sigma$ is close to 0. When $x$ is positive, $\\sigma$ is close to 1. At $x=0$, $\\sigma=0.5$"]},{"cell_type":"code","metadata":{"id":"eWEu55duZHLt","colab_type":"code","colab":{}},"source":["import numpy as np\n","import tensorflow as tf\n","import matplotlib.pyplot as plt\n","import itertools as it\n","\n","import helpers_04\n","\n","%matplotlib inline"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"cyOsepLPZHLw","colab_type":"code","colab":{}},"source":["def show_me(xs, ys, ylim, cross):\n","    fig = plt.figure(figsize=(6,4))\n","\n","    plt.grid(True, which='both')\n","    plt.axhline(y=0, color='y')\n","    plt.axvline(x=0, color='y')\n","    \n","    plt.plot(xs, ys)\n","    plt.plot(0,cross,'ro')\n","\n","    plt.ylim(ylim)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZlRifwogZHLy","colab_type":"code","colab":{}},"source":["xs = np.linspace(-10.0, 10.0, num=100)\n","sigmoid = tf.nn.sigmoid(xs)\n","ys = tf.Session().run(sigmoid)\n","show_me(xs, ys, (-0.1, 1.15), .5)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"G7uRE17NZHL1","colab_type":"text"},"source":["Pros:\n","\n","* Easy derivative\n","* Function looks like we think a neuron might function: it is either off or outputing a value (up to a maximum)\n","\n","Cons: \n","\n","* Not symmetric, which causes issues when training\n","* Susceptible to vanishing gradients: when input values are saturated (either positively or negatively), the derivative is close to zero.\n","\n","##### Derivative\n","\n","Derivative of the sigmoid is easy to calculate if you know the output:\n","\n","$$\n","\\frac{d\\sigma}{dx} = \\sigma \\left(1 - \\sigma \\right)\n","$$\n"]},{"cell_type":"code","metadata":{"id":"6fRdSsYbZHL1","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"IfH4S10jZHL4","colab_type":"text"},"source":["## Hyperbolic Tangent (Tanh)\n","\n","$$\n","tanh(x) = \\frac{sinh(x)}{cosh(x)} = \\frac{e^{x} - e^{-x}}{e^{x} + e^{-x}} = \\frac{e^{2x} - 1}{e^{2x} + 1}\n","$$\n","\n","Pros:\n","* Similar to sigmoid, but \"stretched\" to range from (-1, 1)\n","* Symmetric around 0, which helps for optimization\n","\n","Cons:\n","* Still suffers from vanishing gradient\n","##### Derivative\n","\n","$$\n","\\frac{dtanh}{dx} = 1 - tanh^{2} \n","$$"]},{"cell_type":"code","metadata":{"id":"wQ3FSah3ZHL5","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Z2gk45J3ZHL7","colab_type":"text"},"source":["## Rectified Linear Unit (ReLU)\n","\n","$$\n","ReLU(x) = max\\left(0,x\\right) \\\\ \\\\\n","$$\n","\n","Equivalent to:\n","\n","$$\n","\\begin{align*}\n","  ReLU(x) = \\begin{cases}\n","    0 & \\text{if $x\\lt0$} \\\\\n","    x & \\text{if $x\\geq0$}\n","  \\end{cases}\n","\\end{align*}\n","$$\n","\n","Pros:\n","* Incredibly easy to calculate output and derivative\n","* Doesn't suffer from vanishing gradient on positive side\n","* In practice tend to be more useful than Sigmoid/Tanh for typical activation functions\n","\n","Cons:\n","* Not symmetric\n","* Can cause exploding activations if not careful\n","* Gradient can \"die\" if not careful\n","\n","##### Derivative\n","\n","$$\n","\\begin{align*}\n","  \\frac{dReLU}{dx} = \\begin{cases}\n","    0 & \\text{if $x\\lt0$} \\\\\n","    1 & \\text{if $x\\geq0$}\n","  \\end{cases}\n","\\end{align*}\n","$$"]},{"cell_type":"code","metadata":{"id":"09f8eZqqZHL8","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZKLJ9WIoZHL_","colab_type":"text"},"source":["## Leaky ReLU\n","\n","$$\n","LReLU(x) = max\\left(\\alpha x,x\\right) \\\\ \\\\\n","$$\n","\n","Equivalent to:\n","\n","$$\n","\\begin{align*}\n","  ReLU(x) = \\begin{cases}\n","    \\alpha x & \\text{if $x\\lt0$} \\\\\n","    x & \\text{if $x\\geq0$}\n","  \\end{cases}\n","\\end{align*}\n","$$\n","\n","Pros:\n","* Similar to ReLU, but doesn't \"die\".\n","\n","Cons:\n","* Yet another hyper-parameter to tune.\n","\n","##### Derivative\n","\n","$$\n","\\begin{align*}\n","  \\frac{dReLU}{dx} = \\begin{cases}\n","    \\alpha & \\text{if $x\\lt0$} \\\\\n","    1 & \\text{if $x\\geq0$}\n","  \\end{cases}\n","\\end{align*}\n","$$\n"]},{"cell_type":"code","metadata":{"id":"4Gljv1WvZHMA","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}